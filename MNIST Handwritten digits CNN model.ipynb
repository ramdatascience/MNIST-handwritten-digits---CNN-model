{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train original shape (60000, 28, 28)\n",
      "y_train original shape (60000,)\n",
      "X_test original shape (10000, 28, 28)\n",
      "y_test original shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "#we can load the MNIST dataset from Keras datasets\n",
    "#60.000 training samples and 10.000 images in test set\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape)\n",
    "print(\"X_test original shape\", X_test.shape)\n",
    "print(\"y_test original shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b82785deb8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#let's polit a grayscale image with the label\n",
    "plt.imshow(X_train[2], cmap='gray')\n",
    "plt.title('Class '+ str(y_train[2]))\n",
    "\n",
    "#tensorflow can handle format: (batch,height,width,channel)\n",
    "features_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "features_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "features_train = features_train.astype('float32')\n",
    "features_test = features_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very similar to min-max normalization: we transform the values\n",
    "#within the range [0,1] as usual\n",
    "features_train/=255\n",
    "features_test/=255\n",
    "\n",
    "#we have 10 output classes we want to end up with one hot\n",
    "#encoding as we have seen for the Iris-dataset\n",
    "# 2 -> [0 0 1 0 0 0 0 0 0 0 ]\n",
    "targets_train = np_utils.to_categorical(y_train, 10)\n",
    "targets_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the Convolutional Neural Network (CNN)\n",
    "model = Sequential()\n",
    "\n",
    "#input is a 28x28 pixels image\n",
    "#32 is the number of filters - (3,3) size of the filter\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))\n",
    "model.add(Activation('relu'))\n",
    "#normalizing the activations in the previous layer after the convolutional phase\n",
    "#transformation maintains the mean activation close to 0 std close to 1\n",
    "#the scale of each dimension remains the same\n",
    "#reduces running-time of training significantly\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#flattening layer \n",
    "model.add(Flatten())\n",
    "# Fully connected layer\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "#regularization helps to avoid overfitting\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10,activation=\"softmax\"))\n",
    "#multiclass classification: cross-entropy loss-function with ADAM optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 669s 11ms/step - loss: 0.1012 - acc: 0.9689 - val_loss: 0.0354 - val_acc: 0.9884\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 899s 15ms/step - loss: 0.0346 - acc: 0.9891 - val_loss: 0.0283 - val_acc: 0.9899\n",
      "10000/10000 [==============================] - 41s 4ms/step\n",
      "Test accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "model.fit(features_train, targets_train, batch_size=128, epochs=2, validation_data=(features_test,targets_test), verbose=1)\n",
    "\n",
    "score = model.evaluate(features_test, targets_test)\n",
    "print('Test accuracy: %.2f' % score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
